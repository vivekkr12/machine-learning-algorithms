{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "# Lasso regression\n",
    "\n",
    "*Linear regression with least square loss with $L_1$ regularization penalty*\n",
    "\n",
    "---\n",
    "* [Implementation in Python](../pymlalgo/regression/lasso_regression.py)\n",
    "* [Demo](../demo/lasso_regression_demo.ipynb)\n",
    "---\n",
    "\n",
    "### Symbols and Conventions\n",
    "Refer to [Symbols and Conventions](symbols_and_conventions.ipynb) for details. In summary:\n",
    "* $n$ is the number of training examples\n",
    "* $d$ is the number of features in each training example (a.k.a the dimension of the training example)\n",
    "* $X$ is the features matrix of shape $n \\times d$\n",
    "* $Y$ is the labels matrix of shape $n \\times 1$\n",
    "* $beta$ is the weight matrix of shape $d \\times 1$\n",
    "\n",
    "\n",
    "The cost function for Lasso Regression can be written as**,\n",
    "\n",
    " \n",
    "\n",
    "$$F(\\beta) = \\frac{1}{n} ||Y - X^{T}\\beta||_2^2 + \\lambda||\\beta||_1$$\n",
    "\n",
    "\n",
    "The minimization problem is written as:\n",
    "\n",
    "$$min _{\\beta_j \\in {\\rm I\\!R} ^{1}} F(\\beta) = \\frac{1}{n} ||Y - X^{T}\\beta||_2^2 + \\lambda||\\beta||_1$$\n",
    "\n",
    "where  $j = 1, 2, 3, ..., d$\n",
    "\n",
    "While minimizing w.r.t $\\beta_j$ all the other betas, $\\beta_1, \\beta_2, ..., \\beta_{j-1}, \\beta_{j+1}, ...., \\beta_d$ are held constant.\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "Let's assume the first term of the objective function as $g(\\beta)$ and the second term as $h(\\beta)$\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    "Since $g$ is differentiable, the sub gradient will be equal to the gradient. We can find the derivative using the chain rule:\n",
    "\n",
    " \n",
    "\n",
    "$$\\partial_{\\beta_j} g(\\beta) = \\triangledown _{\\beta_j} (\\beta) = -\\frac{2}{n}X_j(Y-X^T\\beta)$$\n",
    "\n",
    "$$=-\\frac{2}{n}X_j(Y-X_j^T\\beta_j - X_{-j}^{T}\\beta_{-j})$$\n",
    "\n",
    "where $X_{-j}$ and ${\\beta_{-j}}$ are the predictor matrix and the coefficients vector with the $j^{th}$ dimension removed. \n",
    "\n",
    "$$=-\\frac{2}{n}X_j(Y - X_{-j}^{T}\\beta_{-j}) - \\frac{2}{n}X_j(-X_j^T\\beta_j)$$\n",
    "\n",
    "$$=-\\frac{2}{n}X_j(Y - X_{-j}^{T}\\beta_{-j}) - \\frac{2}{n}(-1)X_jX_j^T\\beta_j)$$\n",
    "\n",
    "$$=-\\frac{2}{n}X_j(Y - X_{-j}^{T}\\beta_{-j}) - \\frac{2}{n}(-1)||X_j||_2^2\\beta_j$$\n",
    "\n",
    "$$=-\\frac{2}{n}(X_j(Y - X_{-j}^{T}\\beta_{-j}) - ||X_j||_2^2\\beta_j)$$\n",
    "\n",
    " \n",
    "\n",
    "Let's say that, $z_j = ||X_j||_2^2$ and \n",
    "\n",
    "$R_{-j} = Y - X_{-j}^{T}\\beta_{-j}$. \n",
    "\n",
    "Thus we can write the equation as :\n",
    "\n",
    "$$=-\\frac{2}{n}(X_jR_{-j} - \\beta_jz_j)$$\n",
    "\n",
    " \n",
    "\n",
    "Now, let's tackle $h(\\beta)$\n",
    "\n",
    "$$h(\\beta) = \\lambda||\\beta||_1 = \\sum_{j=1}^{d}|\\beta_j|$$\n",
    "\n",
    "differentiating w.r.t $\\beta_j$, we will only get the derivative w.r.t $\\beta_j$ as all other terms will be 0.\n",
    "\n",
    "$$\\partial _{\\beta_j} h(\\beta) = \\partial _{\\beta_j} (\\lambda|\\beta_j|) = \\begin{cases}\\lambda & \\beta > 0 \\\\\n",
    "-\\lambda & \\beta < 0 \\\\\n",
    "v\\lambda  & \\beta = 0, v \\in [-1, 1] \\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "Using the results and combining $g$ and $h$, we can write\n",
    "\n",
    "$$\\partial_{\\beta_j} F(\\beta) =\\begin{cases}-\\frac{2}{n}(X_jR_{-j} - \\beta_jz_j) + \\lambda & \\beta > 0 \\\\\n",
    "-\\frac{2}{n}(X_jR_{-j} - \\beta_jz_j)-\\lambda & \\beta < 0 \\\\\n",
    "-\\frac{2}{n}(X_jR_{-j} - \\beta_jz_j) + v\\lambda  & \\beta = 0, v \\in [-1, 1] \\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "Equating all the three cases to 0, we get \n",
    "\n",
    "$$\\beta_j =\\begin{cases}\\frac{\\lambda + \\frac{2}{n}x_j R_{-j}}{\\frac{2}{n}z_j} & \\frac{2}{n}x_iR_{-j} \\le -\\lambda \\\\\n",
    "\\frac{-\\lambda + \\frac{2}{n}x_j R_{-j}}{\\frac{2}{n}z_j} & \\frac{2}{n}x_iR_{-j} \\ge \\lambda \\\\\n",
    "0 & |\\frac{2}{n}x_iR_{-j}| \\ge \\lambda\\\\\n",
    "\\end{cases}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
