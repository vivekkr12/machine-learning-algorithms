{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA\n",
    "\n",
    "*Dimensionality Reduction using Eigenvalue Decomposition*\n",
    "\n",
    "---\n",
    "* [Implementation in Python](../pymlalgo/pca/pca.py)\n",
    "* [Demo](../demo/pca_demo.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## Symbols and Conventions\n",
    "Refer to [Symbols and Conventions](symbols_and_conventions.ipynb) for details. The following list contains the summary and the symbols specific to *PCA*\n",
    "* $n$ is the number of training examples\n",
    "* $d$ is the number of features in each training example (a.k.a the dimension of the training example)\n",
    "* $X$ is the features matrix of shape $n \\times d$\n",
    "* $Z$ is the centered $X$ matrix i.e. $z_j = x_{ij} - \\bar{x_j}$ where $i \\in \\{1, 2, ..., n\\}$ and $j \\in \\{1, 2,..., d\\}$\n",
    "     * $x_j$ is one column of $X$ matrix that is one feature of all training examples\n",
    "     * $x_{ij}$ is the $j^{th}$ feature of $i^{th}$ training example\n",
    "     * $\\bar{x_j}$ is the mean of $x_j$\n",
    "* $A$ is the covariance matrix of $Z$ of shape $d \\times d$\n",
    "    * $A = \\frac{1}{n - 1}Z^TZ$\n",
    "* $\\lambda_j$ is the $j^{th}$ eigenvalue\n",
    "* $v_j$ is the $j^{th}$ eigenvector\n",
    "\n",
    "## Eigenvalues and Eigenvectors\n",
    "Any diagonalizable matrix, $A$ can be written in the form\n",
    "$$Av = \\lambda v$$\n",
    "where $\\lambda$ is called the eigenvalue $v$ is called the eigenvector.\n",
    "* $A$ must be a square matrix, lets say the shape is $d \\times d$, thus\n",
    "* $v$ must be of shape $d \\times 1$\n",
    "* $\\lambda$ is a scalar\n",
    "\n",
    "The equation has multiple solutions, however for dimensionality reduction, the largest eigenvector associated with the largest eigenvalue explains the maximum variance.\n",
    "\n",
    "## Power Iteration\n",
    "Power iteration is an algorithm to calculate the largest (in absolute value) eigenvalue for a diagonalizable matrix.\n",
    "\n",
    "1. Input the matrix $A$\n",
    "2. Initialize the vector $v_0$ of shape $d \\times 1$\n",
    "3. For $t = \\{1, 2, 3, ...\\}$ apply the updates until stopping criteria not satisfied\n",
    "    1. $v_t = Av_{t-1}$\n",
    "    2. $v_t = \\frac{v_t}{||v_t||_2}$\n",
    "    3. $\\lambda_t = v_t^TAv_t$\n",
    "\n",
    "### Stopping Criteria\n",
    "The stopping criteria can be either of the three or their combination whichever is satisfied first\n",
    "1. Maximum number of iterations\n",
    "2. $||Av - \\lambda v||_2 < \\epsilon$ where $\\epsilon$ is a very small positive value. Since power iteration optimizes the loss given by the equation, as soon as the loss reaches a very small value, the algorithm has converged.\n",
    "3. $||v_t - v_{t-1}||_2 < \\epsilon$ where $\\epsilon$ is a very small value\n",
    "\n",
    "### Dimensionality Reduction\n",
    "Once the eigenvector, $v$ is computed, the dimension of the training examples can be reduced to $n \\times 1$ by taking the dot product $Zv$. This dot product is known as the first principal component.\n",
    "\n",
    "### $2^{nd}, 3^{rd}, ..., d^{th}$ Principal Component\n",
    "To find the second principal component, the variance explained by first principal component is subtracted from $A$.\n",
    "$$A_2 = A_1 - \\lambda_1  v_1 v_1^T$$\n",
    "Here, the subscript $_1$ represents the first (largest) eigenvector. Now power iteration is applied to the resulting matrix, $A_2$\n",
    "\n",
    "### PCA Using Power Iteration\n",
    "Joining all the moving parts, $X$ can be reduced to $d_{reduced}$ dimensions using the following algorithm\n",
    "1. Input $X$ and $d_{reduced} < d$\n",
    "2. Compute centered matrix, $Z = X - \\bar{X}$ (Note: $\\bar{X}$ is of shape $d \\times 1$ and is the mean of each feature across all training examples\n",
    "3. Compute the covariance matrix $A = \\frac{1}{n - 1} Z^TZ$\n",
    "4. Repeat for $j = \\{1, 2, ,3, ....., d_{reduced}\\}$\n",
    "    1. Compute $v_j$ and $\\lambda_j$ using power iteration\n",
    "    2. Update $A_j = A_{j-1} - \\lambda_j v_j v_j^T$\n",
    "    3. Compute $j^{th}$ principal component $X_{pj} = Zv_j$\n",
    "\n",
    "\n",
    "## Normalized Oja Algorithm\n",
    "Normalized Oja algorithm is another method to find principal components. The algorithm also takes a diagonalizable matrix and computes the largest eigenvector\n",
    "1. Input diagonalizable matrix $A$ and learning rate $\\eta$ (Note: Here $A$ is not the covariance matrix)\n",
    "2. Initialize vector $v_0$ of shape $d \\times 1$\n",
    "3. Update $v_0 = \\frac{v_0}{||v_0||_2}$\n",
    "4. For $t = \\{1, 2, 3, ...\\}$ apply the updates until stopping criteria not satisfied\n",
    "    1. $v_t =\\eta Av_{t-1}$\n",
    "    2.  $v_t = \\frac{v_t}{||v_t||_2}$\n",
    "\n",
    "### Stopping Criteria\n",
    "Stopping criteria $1$ & $3$ can be used for Oja algorithm\n",
    "\n",
    "### Principal Components\n",
    "Similar to Power Iteration, to compute the principal component, the following rule is applied to project the eigenvectors one dimensional space:\n",
    "$$X_{p1} = Zv$$\n",
    "\n",
    "### $2^{nd}, 3^{rd}, ..., d^{th}$ Principal Component\n",
    "The first principal component is projected to $d$-dimensions and then subtracted from the centered features matrix $Z$. Diagonalizable matrix $A$ is then calculated from the resultant $Z$ and then Oja algorithm is applied to it:\n",
    "$$Z_2 = Z_1 - Z_1v_1v_1^T$$\n",
    "$$A_2 = Z_2^TZ_2$$\n",
    "\n",
    "### PCA Using Oja Algorithm\n",
    "Collecting all the steps together, $X$ can be reduced to $d_{reduced}$ dimensions using the following algorithm:\n",
    "1. Input $X$ and $d_{reduced} < d$\n",
    "2. Compute centered matrix, $Z = X - \\bar{X}$ (Note: $\\bar{X}$ is of shape $d \\times 1$ and is the mean of each feature across all training examples\n",
    " 3. Compute $A =  Z^TZ$\n",
    "4. Repeat for $j = \\{1, 2, ,3, ....., d_{reduced}\\}$\n",
    "    1. Compute $v_j$ using Oja Algorithm\n",
    "    2. Compute $j^{th}$ principal component $X_{pj} = Zv_j$\n",
    "    3. Update $Z_j = Z_{j-1} - Z_{j-1}v_jv_j^T$\n",
    "    4. Update $A_j = Z_j^TZ_j$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
